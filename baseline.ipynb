{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "In this notebook we prepare a simple solution.\n",
    "\n",
    "Note: There is dependence on `rep` library. For its installation use `pip install rep --no-dependencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from rep.utils import train_test_split_group\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "* Create the folder `datasets`\n",
    "* Download files from kaggle to this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('datasets/training.csv')\n",
    "test = pandas.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Corrected_mass</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Pt_sum</th>\n",
       "      <th>Pt_min</th>\n",
       "      <th>IP_chi2</th>\n",
       "      <th>IP_chi2_sum</th>\n",
       "      <th>Flight_distance</th>\n",
       "      <th>Pseudorapidity</th>\n",
       "      <th>Track_number_PV</th>\n",
       "      <th>Tracks_number</th>\n",
       "      <th>Tracks_number_passed</th>\n",
       "      <th>Vertex_chi2</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1525.020022</td>\n",
       "      <td>4209.730022</td>\n",
       "      <td>2354.220022</td>\n",
       "      <td>2670.780022</td>\n",
       "      <td>242.804022</td>\n",
       "      <td>9.304122</td>\n",
       "      <td>37.116622</td>\n",
       "      <td>32.554622</td>\n",
       "      <td>4.205552</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.833562</td>\n",
       "      <td>2.849395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5111.200004</td>\n",
       "      <td>5936.160004</td>\n",
       "      <td>3599.690004</td>\n",
       "      <td>5411.700004</td>\n",
       "      <td>420.789004</td>\n",
       "      <td>7.434904</td>\n",
       "      <td>1805.900004</td>\n",
       "      <td>1736.040004</td>\n",
       "      <td>3.195634</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.750624</td>\n",
       "      <td>2.849395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4799.079977</td>\n",
       "      <td>5751.109977</td>\n",
       "      <td>3929.529977</td>\n",
       "      <td>5233.069977</td>\n",
       "      <td>246.445977</td>\n",
       "      <td>7.428827</td>\n",
       "      <td>1300.089977</td>\n",
       "      <td>1301.229977</td>\n",
       "      <td>3.234167</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6.719367</td>\n",
       "      <td>2.849395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4080.210007</td>\n",
       "      <td>5156.490007</td>\n",
       "      <td>4013.150007</td>\n",
       "      <td>4972.080007</td>\n",
       "      <td>536.857007</td>\n",
       "      <td>34.544807</td>\n",
       "      <td>1290.930007</td>\n",
       "      <td>1329.710007</td>\n",
       "      <td>3.215467</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.902209</td>\n",
       "      <td>2.849395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3828.379986</td>\n",
       "      <td>5094.629986</td>\n",
       "      <td>3989.839986</td>\n",
       "      <td>4860.719986</td>\n",
       "      <td>422.611986</td>\n",
       "      <td>57.246786</td>\n",
       "      <td>1512.289986</td>\n",
       "      <td>1497.489986</td>\n",
       "      <td>3.196906</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.682696</td>\n",
       "      <td>2.849395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventID  Label         Mass  Corrected_mass           Pt       Pt_sum  \\\n",
       "0        0      1  1525.020022     4209.730022  2354.220022  2670.780022   \n",
       "1        0      1  5111.200004     5936.160004  3599.690004  5411.700004   \n",
       "2        0      1  4799.079977     5751.109977  3929.529977  5233.069977   \n",
       "3        0      1  4080.210007     5156.490007  4013.150007  4972.080007   \n",
       "4        0      1  3828.379986     5094.629986  3989.839986  4860.719986   \n",
       "\n",
       "       Pt_min    IP_chi2  IP_chi2_sum  Flight_distance  Pseudorapidity  \\\n",
       "0  242.804022   9.304122    37.116622        32.554622        4.205552   \n",
       "1  420.789004   7.434904  1805.900004      1736.040004        3.195634   \n",
       "2  246.445977   7.428827  1300.089977      1301.229977        3.234167   \n",
       "3  536.857007  34.544807  1290.930007      1329.710007        3.215467   \n",
       "4  422.611986  57.246786  1512.289986      1497.489986        3.196906   \n",
       "\n",
       "   Track_number_PV  Tracks_number  Tracks_number_passed  Vertex_chi2    Weight  \n",
       "0                1              2                     1     5.833562  2.849395  \n",
       "1                0              4                     2     1.750624  2.849395  \n",
       "2                1              4                     2     6.719367  2.849395  \n",
       "3                0              3                     2     0.902209  2.849395  \n",
       "4                0              3                     2     2.682696  2.849395  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training features\n",
    "\n",
    "Exclude `EventID`, `Label` and `Weight` from the features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP_chi2_sum',\n",
       " 'Flight_distance',\n",
       " 'Pt',\n",
       " 'Tracks_number',\n",
       " 'Pt_sum',\n",
       " 'Corrected_mass',\n",
       " 'Track_number_PV',\n",
       " 'Pseudorapidity',\n",
       " 'Tracks_number_passed',\n",
       " 'Mass',\n",
       " 'IP_chi2',\n",
       " 'Pt_min',\n",
       " 'Vertex_chi2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(set(data.columns) - {'EventID', 'Label', 'Weight'})\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide training data into 2 parts\n",
    "Here `train_test_split_group` function is used to divide into 2 parts to preserve secondary vertices from the same events in the same part of data (training or test). First argument should be events ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split_group(data.EventID, data, random_state=11, train_size=0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    result = data.copy()\n",
    "    result[\"log_Pt_sum\"] = np.log(result[\"Pt_sum\"])\n",
    "    result[\"log_Flight_distance\"] = np.log(result[\"Flight_distance\"])\n",
    "    result[\"log_Pt\"] = np.log(result[\"Pt\"])\n",
    "    result[\"log_Pt_min\"] = np.log(result[\"Pt_min\"])\n",
    "    result[\"log_IP_chi2_sum\"] = np.log(result[\"IP_chi2_sum\"])\n",
    "    result[\"log_IP_chi2\"] = np.log(result[\"IP_chi2\"])\n",
    "    result[\"log_Corrected_mass\"] = np.log(result[\"Corrected_mass\"])\n",
    "    result[\"log_Mass\"] = np.log(result[\"Mass\"])\n",
    "    return result\n",
    "    \n",
    "features = features + [\"log_Pt_sum\", \"log_Flight_distance\", \"log_Pt\", \"log_Pt_min\", \n",
    "                       \"log_IP_chi2_sum\", \"log_IP_chi2\", \"log_Corrected_mass\", \"log_Mass\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = prepare_data(training_data)\n",
    "validation_data = prepare_data(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple gradient boosting from `sklearn` training\n",
    "\n",
    "We take all secondary vertices (SVs) for all events and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_fit(training_data):\n",
    "    gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=200, subsample=0.8, random_state=13,\n",
    "                                min_samples_leaf=200, max_depth=8, max_features=10)\n",
    "    gb.fit(training_data[features], training_data.Label)\n",
    "\n",
    "    proba = gb.predict_proba(training_data[features])\n",
    "\n",
    "    top = pandas.DataFrame({\"EventID\": training_data.EventID.values,\n",
    "                  \"index\": training_data.EventID.index,\n",
    "                  \"proba\": proba[:,1]}).groupby(\"EventID\").apply(lambda x: x.nlargest(3, \"proba\"))\n",
    "\n",
    "    top_values = top[\"index\"].values\n",
    "\n",
    "    top_data = training_data.loc[top_values]\n",
    "\n",
    "    gb2 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=200, subsample=0.8, random_state=13,\n",
    "                                min_samples_leaf=50, max_depth=8, max_features=10)\n",
    "    gb2.fit(top_data[features], top_data.Label)\n",
    "    \n",
    "    proba = gb2.predict_proba(training_data[features])\n",
    "\n",
    "    top = pandas.DataFrame({\"EventID\": training_data.EventID.values,\n",
    "                  \"index\": training_data.EventID.index,\n",
    "                  \"proba\": proba[:,1]}).groupby(\"EventID\").apply(lambda x: x.nlargest(3, \"proba\"))\n",
    "\n",
    "    top_values = top[\"index\"].values\n",
    "\n",
    "    top_data = training_data.loc[top_values]\n",
    "\n",
    "    gb3 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=200, subsample=0.8, random_state=13,\n",
    "                                min_samples_leaf=50, max_depth=8, max_features=10)\n",
    "    gb3.fit(top_data[features], top_data.Label)\n",
    "    \n",
    "    return gb2, gb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb2, gb3 = do_fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare predictions, labels and weights for events (not for SVs!) on the cross-validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean(event_ids, values):\n",
    "    \"\"\" fore each event computes average of given values \"\"\"\n",
    "    number_of_sv_in_event = np.bincount(event_ids)\n",
    "    return np.bincount(event_ids, weights=values) / number_of_sv_in_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_max(values_event_ids, values, event_ids):\n",
    "    \"\"\" fore each event computes average of given values \"\"\"\n",
    "    result = np.zeros(len(event_ids))\n",
    "    for i, event in enumerate(event_ids):\n",
    "        result[i] = np.max(values[values_event_ids==event])\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict each SV\n",
    "events_ids = np.unique(validation_data.EventID)\n",
    "\n",
    "# compute number of SVs in each event\n",
    "number_of_sv_in_event = np.bincount(validation_data.EventID)\n",
    "\n",
    "# compute predictions for events (take the mean value of predictions for SVs forming an event)\n",
    "proba1 = gb2.predict_proba(validation_data[features])\n",
    "proba2 = gb3.predict_proba(validation_data[features])\n",
    "\n",
    "proba = (proba1 + proba2) / 2\n",
    "\n",
    "events_proba = compute_max(validation_data.EventID.values, proba[:,1], events_ids)\n",
    "\n",
    "# compute weights for events \n",
    "events_weights = compute_mean(validation_data.EventID, validation_data.Weight)[events_ids]\n",
    "\n",
    "# compute labels for events \n",
    "events_labels = compute_mean(validation_data.EventID, validation_data.Label)[events_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC for events (with weights) on the cross validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96629050509855519"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(events_labels, events_proba, sample_weight=events_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = prepare_data(data)\n",
    "\n",
    "gb2, gb3 = do_fit(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_data(test)\n",
    "# predict each SV in test sample\n",
    "kaggle_proba1 = gb2.predict_proba(test[features])\n",
    "kaggle_proba2 = gb3.predict_proba(test[features])\n",
    "\n",
    "kaggle_proba = (kaggle_proba1 + kaggle_proba2) / 2\n",
    "\n",
    "kaggle_ids = np.unique(test.EventID)\n",
    "# compute predictions for events (take the mean value of predictions for SVs forming an event)\n",
    "kaggle_events_proba = compute_max(test.EventID.values, kaggle_proba[:, 1], kaggle_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='datasets/baseline.csv' target='_blank'>datasets/baseline.csv</a><br>"
      ],
      "text/plain": [
       "/Users/atsky/work/LHC/datasets/baseline.csv"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "def create_solution(ids, proba, filename='baseline.csv'):\n",
    "    \"\"\"saves predictions to file and provides a link for downloading \"\"\"\n",
    "    pandas.DataFrame({'EventID': ids, 'Label': proba}).to_csv('datasets/{}'.format(filename), index=False)\n",
    "    return FileLink('datasets/{}'.format(filename))\n",
    "    \n",
    "create_solution(kaggle_ids, kaggle_events_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
